Task 2
# Веб-краулер

Python-скрипт для анализа структуры веб-сайтов, сбора статистики и обнаружения ресурсов.

## Возможности

- Рекурсивный обход сайтов до указанной глубины
- Классификация ссылок (внутренние/внешние)
- Обнаружение поддоменов
- Поиск документов (PDF, DOC, DOCX)
- Сбор детальной статистики о сайте
- Корректная обработка редиректов и битых ссылок

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/вашusername/web-crawler.git
cd web-crawler
```

## Запуск

python crawler.py [URL] [--deep ГЛУБИНА]

### Обход example.com со стандартной глубиной (10 страниц)
python crawler.py https://example.com

### Обход с указанием глубины (50 страниц)
python crawler.py https://example.com --deep 50
